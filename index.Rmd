---
title: "Open Case Studies: Exploring global patterns of obesity across rural and urban regions"
css: style.css
output:
  html_document:
    self_contained: yes
    code_download: yes
    highlight: tango
    number_sections: no
    theme: cosmo
    toc: yes
    toc_float: yes
  pdf_document:
    toc: yes
  word_document:
    toc: yes
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(include = TRUE, comment = NA, echo = TRUE,
                      message = FALSE, warning = FALSE, cache = FALSE,
                      fig.align = "center", out.width = '90%')
library(here)
library(knitr)
```

 

```{r, echo = FALSE}
knitr::include_graphics(here::here("img", "mainplot.png"))
```

## {.disclaimer_block}

**Disclaimer**: The purpose of the [Open Case Studies](https://opencasestudies.github.io) project is **to demonstrate the use of various data science methods, tools, and software in the context of messy, real-world data**. A given case study does not cover all aspects of the research process, is not claiming to be the most appropriate way to analyze a given dataset, and should not be used in the context of making policy decisions without external consultation from scientific experts. 

## Motivation

Body mass index ([BMI](https://www.cdc.gov/healthyweight/assessing/bmi/adult_bmi/index.html)) is often used as a proxy for **adiposity** and is measured as an individual's weight in kilograms (kg) or pounds (lbs) divided by the individual's height in meters ($m^2$) squared.
There are different categories of weight defined using different BMI thresholds as defined by the [World Health Organization](http://www.euro.who.int/en/health-topics/disease-prevention/nutrition/a-healthy-lifestyle/body-mass-index-bmi) (WHO), the [Centers for Disease Control and Prevention](https://www.cdc.gov/obesity/adult/defining.html), and the [National Institutes of Health](https://www.nhlbi.nih.gov/sites/default/files/media/docs/obesity-evidence-review.pdf). 
For example, these are the categories defined by the WHO: 

```{r, echo=FALSE}
knitr::include_graphics("https://www.ncbi.nlm.nih.gov/books/NBK535456/bin/bmi_WHO.jpg")
```

###### [[source](https://www.ncbi.nlm.nih.gov/books/NBK535456/bin/bmi_WHO.jpg)]

The following chart can help you identify your estimate of BMI:
```{r, echo=FALSE}
knitr::include_graphics("http://content.everydayhealth.com/mayodiet/images/BMI-chart.jpg")
```

###### [[source](http://diet.mayoclinic.org/diet/eat/what-is-your-bmi?xid=nl_MayoClinicDiet_20160426)]

Previous work has shown higher BMI (>30) is associated with [higher rates of all-causes of mortality](https://www.nhlbi.nih.gov/sites/default/files/media/docs/obesity-evidence-review.pdf), as well as increased rates of type 2 diabetes, cancer, heart disease, and stroke. 

Recently, an [article](https://www.nature.com/articles/s41586-019-1171-x.pdf) published in [*Nature*](https://www.nature.com) evaluated and compared the Body-Mass Index (BMI) of populations in rural and urban communities around the world: 

```{r,  echo=FALSE, out.width='100%'}
knitr::include_graphics(here::here("img", "paper.png"))
```

#### {.reference_block}
NCD Risk Factor Collaboration (NCD-RisC). Rising rural body-mass index is the main driver of the global obesity epidemic in adults. *Nature* **569**, 260–264 (2019). 

####

The article challenged the widely-held view that increased urbanization was one of the major reasons for increased global obesity rates. 
This view came about because many countries around the world have shown increased urbanization levels in parallel with increased obesity rates. 
However, the NCD-RisC argued that this might not be the case; and that in fact for most regions around the world, BMI measurements are increasing in rural populations just as much, if not more so, than urban populations. 
Furthermore, this study suggested that obesity has particularly increased in female populations in rural regions: 

> "We noted a persistently higher rural BMI, especially for women."

In this case study, we will evaluate the data reported in this article to explore regional and gender specific differences in the obesity rates around the world in 1985 and 2017. 
Most importantly we will test if there is a difference in obesity rates between rural and urban communities and if there has been a change in obesity over time for these regions, particularly for females. 
To do this we will test if there is a difference between the average BMI for each group.

### Main Questions
#### {.main_question_block}

<b><u> Our main questions are: </u></b>

1) Is there a difference between rural and urban BMI estimates around the world? In particular, how does this look for females?
2) How have BMI estimates changed from 1985 to 2017? Again, In particular, how does this look for females?
3) How do different countries vary in terms of estimates of BMI? In particular, how does the United States compare to the rest of the world?

####

### Learning Objectives 

In this case study, we will walk you through importing data from a pdf, cleaning, wrangling and visualizing the data, and <b> comparing two groups </b> using well-established and commonly used packages, including `stringr`, `tidyr`, `dplyr`, `purrr`, and `ggplot2` from the [`tidyverse`](https://www.tidyverse.org/). 
Specifically, the tidyverse is

> "an opinionated collection of R packages designed for data science. All packages share an underlying philosophy and common APIs".

Another way of putting it is that it's a set of packages that are useful specifically for data manipulation, exploration and visualization with a common philosophy.
While some students may be familiar with previous packages from the R programming language, these packages make data science in R especially efficient.

```{r, echo = FALSE, out.width = "20%"}
knitr::include_graphics("https://tidyverse.tidyverse.org/logo.png")
```

We will begin by loading the packages that we will need with a brief explanation:

```{r}
library(here)
library(pdftools)
library(stringr)
library(readr)
library(dplyr)
library(tibble)
library(magrittr)
library(glue)
library(purrr)
library(tidyr)
library(ggplot2)
library(ggrepel)
library(cowplot)
library(patchwork)
```


 Package   | Use                                                                         
---------- |-------------
[here](https://github.com/jennybc/here_here)       | to easily load and save data with relative paths
[pdftools](https://cran.r-project.org/web/packages/pdftools/pdftools.pdf)   | to read a text from pdf into R   
[stringr](https://stringr.tidyverse.org/articles/stringr.html)    | to manipulate the text data
[readr](https://readr.tidyverse.org/)      | to manipulate the text data within the pdf into individual lines  
[dplyr](https://dplyr.tidyverse.org/)      | to arrange/filter/select subsets of the data 
[tibble](https://tibble.tidyverse.org/)     | to create data objects that we can manipulate with `dplyr`/`stringr`/`tidyr`/`purrr`
[magrittr](https://magrittr.tidyverse.org/articles/magrittr.html)   | to use the `%<>%` piping operator
[glue](https://www.tidyverse.org/blog/2017/10/glue-1.2.0/)  | to paste or combine character strings and data together
[purrr](https://purrr.tidyverse.org/)      | to perform functions on all columns of a tibble
[tidyr](https://tidyr.tidyverse.org/)      | to convert data from 'wide' to 'long' format
[ggplot2](https://ggplot2.tidyverse.org/)    | to make visualizations with multiple layers
[ggrepel](https://cran.r-project.org/web/packages/ggrepel/vignettes/ggrepel.html)    | to allow labels in figures not to overlap
[cowplot](https://cran.r-project.org/web/packages/cowplot/vignettes/introduction.html) and [patchwork](https://github.com/thomasp85/patchwork) | to allow plots to be combined 


___

The first time we use a function, we will use the `::` to indicate which package we are using (e.g. `stringr::str_detect()`). 
Unless we have overlapping function names, this is not necessary, but we will include it here to be informative about where the functions we will use come from.

### Context

The measurement of BMI has some [limitations](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4890841/pdf/nt-50-117.pdf) that are well recognized, as it does not account for the composition of body mass, the location of body fat, or the contribution of body frame size. 
However, [BMI has been a useful health indicator](https://journals.lww.com/acsm-healthfitness/Fulltext/2016/07000/THE_BENEFITS_OF_BODY_MASS_INDEX_AND_WAIST.8.aspx#pdf-link) for risk for many diseases and conditions particularly when combined with other risk factor information.

## What are the data?

We will be using data located within a table of the [supplementary material](https://static-content.springer.com/esm/art%3A10.1038%2Fs41586-019-1171-x/MediaObjects/41586_2019_1171_MOESM1_ESM.pdf) for the NCD-RisC paper referenced above. 
This is a pdf that can be found freely available online. Here is a screenshot of the first page of the PDF: 

```{r, echo = FALSE}
knitr::include_graphics(here::here("img","first_page.png"))
```

You can see that the data contain average BMI estimates for both men and women in countries at the national level, as well as the average BMI estimates for the rural and urban areas of these countries in both 1985 and 2017.

The data within the parentheses are the 95% [credible intervals](https://en.wikipedia.org/wiki/Credible_interval) (CIs) for the average BMI estimates. The authors provide these CIs as a guide to understand how likely the estimate is for the true population mean BMI. A wider range suggests that the estimate is less accurate, as there are more possible values for the true mean with credible evidence.

<u>Note:</u> While [gender](https://www.genderspectrum.org/quick-links/understanding-gender/) and [sex](https://www.who.int/genomics/gender/en/index1.html) are not actually binary, the data presented that is used in this analysis only contain data for groups of individuals described as men or women. 

## Data Import

First let's download the data from the PDF file. 
We use `file.exists()` from base R to check if the file we want to download already exists. 
If not, then we download it using the function `utils::download.file()`. 

```{r}
if(!file.exists(here("docs", "paper_supplement.pdf"))){
  url <- "https://static-content.springer.com/esm/art%3A10.1038%2Fs41586-019-1171-x/MediaObjects/41586_2019_1171_MOESM1_ESM.pdf"
  utils::download.file(url, here("docs", "paper_supplement.pdf"))
}
```

Now that we have downloaded the PDF file, we will read it in to R using the `pdftools` package:

```{r}
pdf_obesity <- pdftools::pdf_text(here("docs", "paper_supplement.pdf"))
```


Let's take a look at the data -- the `summary()` function from base R helps us to look at the structure of R objects.

```{r}
summary(pdf_obesity)
```

We can see that we have 63 elements that are character strings. 
You may also notice that the original PDF has 63 pages. Let's take a look at some of these elements.

```{r}
pdf_obesity[1] # this looks like the first page
pdf_obesity[2] # this looks like the second page
```

```{r, eval = TRUE}
str(pdf_obesity[52], nchar.max = 1600)
```
Yes, this looks like data in a the table we want. 
We are using the `str()` function to print just a portion of the data. 
Note `print(pdf_obesity[52])` would show the entire contents of the first page of the table. 

Here is the PDF version again:
```{r, echo = FALSE}
knitr::include_graphics(here::here("img","first_page.png"))
```

We can see that the output looks pretty similar to the pages of the pdf, but the spacing is a bit awkward. 
Note that the way the data is displayed is partially influenced by the width setting of the RStudio window.  

Now, we are interested in a supplementary Table 3, which has multiple pages and includes the same header on each page. 
We can use that to determine what elements of our `pdf_obesity` character strings include our table. 
We will use the `str_detect()` function from the `stringr` package to search for the elements that contain text that is consistently in the header. 
The output of of this function will show which elements of the object (in this case pages of the pdf) include this pattern indicated as a `TRUE` or `FALSE`.

```{r}
stringr::str_detect(pattern = "Age-standardised mean BMI", 
                    string = pdf_obesity)
```
We note, the "Age-standardised mean BMI" is part of the header in the table on every page.
The code above shows the pages that contain our table of interest (as `TRUE` values).

Now we will extract just the data for the table now and call it `rural_urban`. 
To do this we will use the `str_subset()` function of the `stringr` package.

```{r}
rural_urban <- stringr::str_subset(pattern = "Age-standardised mean BMI", 
                                   string = pdf_obesity)

summary(rural_urban) 
```
Similar to the `summary()` function, the `str()` function compactly displays the structure of arbitrary R objects. 
The `nchar.max` argument let's us define the maximal number of characters to show. 
Here, we see there are 10 pages worth of elements in our `rural_urban` object. 

Let's check the first and last page:

```{r, eval=TRUE}
str(rural_urban[1], nchar.max = 1600)
```
Using `[1]` shows just the first page of the 10 pages that contain the table. 
Also, notice the new line `"\n"` values that indicate new lines. 

```{r, echo = FALSE, eval = FALSE}
knitr::include_graphics(here::here("img","first_page_in_R.png"))
```


```{r, echo = FALSE}
knitr::include_graphics(here::here("img","first_page.png"))
```

This looks the same as the beginning... how about the end?

```{r}
str(rural_urban[10], nchar.max = 1800)
```

```{r, echo = FALSE }
knitr::include_graphics(here::here("img","last_page.png"))
```


Great! Our `rural_urban` object looks like it contains the entire Supplementary 3 table, as both the beginning and the end include the data we expected.

## Data Wrangling

At this point we have large strings now for each page of the table, but this is not very convenient to work with. 
Now, we will wrangle the data into a more usable form. 
Ideally, we would like our data to be in some sort of tabular form.

### Separate the data into lines

First, it would be useful to separate the data that is currently in 10 giant string chunks into individual lines or rows of the table. To do this we can use the `read_lines()` function of the `readr` package which will separate the lines based on the `"\n"` new line characters.
```{r}
summary(rural_urban)
rural_urban <- readr::read_lines(rural_urban)
summary(rural_urban) 
# now we have 461 lines
```
We can see that each string in the `rural_urban` object is now a single line of the table.

For example, in line 6 shows the data for females in Afghanistan.
```{r}
rural_urban[6] 
```


### Removing excess white-space

We also have a lot of white-space... let's get rid of the excess white spaces using `str_squish()` also from the `stringr` package.

```{r}
rural_urban <- stringr::str_squish(rural_urban) 
head(rural_urban)
```

Now it is much easier to see the data.

If we look at the end of the first page of the table and the start of the second we can see that the header information is repeated, as well as a line with the page number and an empty line, and a line that says "2 2".

#### {.scrollable }
```{r}
rural_urban[44:56] # scroll through!
```
####

Although the header was necessary on all of the pages of the pdf version of the table, we only need that information once in our data.

### Removing unnecessary repeated header information

So, let's remove all the header information and the page number lines from the `rural_urban` object, then we will make a single line header for the beginning.
One way to do this is to find all lines that include either "Women" or "Men" and only keep this data.

First, let's see how many lines include "Women" or "Men". 
We can do this by first identifying the lines that contain these patterns and then check the length of the resulting vector for the line numbers that contain these patterns. 
Note, the `"|"` indicates that we are looking for either "Women" or "Men".

#### {.scrollable }
```{r}
#scroll through the output!
stringr::str_which(string = rural_urban, 
                   pattern = "Women|Men")
```
####

```{r}
length(stringr::str_which(string = rural_urban, 
                          pattern = "Women|Men"))

```

Note that `str_which()` is case sensitive, so it would not work to use "women" as the pattern, and using "men" would return the lines that contain "Wo**men**" or "Ye**men**" etc.

```{r}
stringr::str_which(string = rural_urban, 
                   pattern = "women")
```

OK, so this looks correct. 
This includes most lines but there are gaps where the header is located. It looks like there are 400 lines in our table that aren't headers.

```{r}
rural_urban <- str_subset(string = rural_urban, 
                          pattern = "Women|Men")
```

We can check our data now using either `head()` from base R or `glimpse()` from the `tibble` package.
```{r}
dplyr::glimpse(rural_urban)
```

Great, now our `rural_urban` object 400 lines instead of 461, like it did before when it contained the redundant header information.

```{r}
head(rural_urban)
```

The `head()` function shows us the first rows or lines of the data, while the `glimpse()` function provides us information about the total size of the object and shows us the first line or row.

Great! So now our data looks much better but we need to add back our header and we would like this to only be a single line to make it easy to transform our data into a table or table-like object.

Here is what our header used to look like:

```{r, echo = FALSE,  out.width = '100%'}
knitr::include_graphics(here::here("img","last_page.png"))
```

### Dealing with spacing

First let's try splitting our header-less data into columns based on spaces using the `str_split()` function, where we specify that we are splitting the data based on the pattern of a space (the space is included in quotes):

Here, we will take a look at just the first 10 lines using the `[1:10,]` syntax: 


#### {.scrollable }
```{r}
str_split(string = rural_urban, 
          pattern = " ", 
          simplify = TRUE)[1:10,] 
#scroll through the output!
```
####

This almost worked, but unfortunately country names that have spaces will be a problem. 
For example, we can see that "American Samoa" has been divided into two columns and all subsequent columns are shifted.

Let's try this another way.
Instead, let's try to extract the country information by separating the country information from the sex information when the sex is female.
To do this, we start by noticing that the sex always starts with either a capital "W" if the gender is female. Note, we need to use a space before the "W" otherwise we will split some of the country names if the names starts with "W". 

Here, we will also introduce the concept of piping, which uses the `%>%` from the `magrittr` package, which allows us to pipe the output from one function to input to another function.
This is really useful when we have multiple steps, which we will show soon.

First, we will select just the data for Women
```{r}
Women <- str_subset(string = rural_urban, 
                    pattern = "Women") 
```

And then split this data based on the pattern `pattern = " Women"` (Note that the space is included - so that it is not within the country data).
```{r}
country_split <- 
  Women %>%
  stringr::str_split(pattern = " Women") %>%
  unlist() 
```

However, we can also do the same thing with two pipes:

1. First, we select the "Women" and then we split the data by " Women"
2. Second, we assign the output of both steps to the object country_split
```{r}
country_split <- 
  str_subset(string = rural_urban,
             pattern = "Women") %>%
  stringr::str_split(pattern= " Women") %>%
  unlist()

head(country_split)
```

Now, we can see that `Country` is always the odd rows and the rest of the data is the rest of the rows. 
We can select the odd rows using the <b> Modulus (Remainder from division) operator</b>. 
This operator looks like this `%%`. 
All odd values such as 3, 7, or 15 when divided by 2 would leave a remainder of 1. 
We will select just these rows using the `filter()` function of `dplyr`.

```{r}
country <- 
  tibble(country_split) %>% 
  dplyr::filter(row_number() %% 2 == 1) 
```

We can take a look to make sure that all the country names look as expected. You can scroll through the country names if you hover over the data.

```{r, eval= FALSE}
country
```

#### {.scrollable }
```{r, echo = FALSE}
as.data.frame(country)
```
####

Looks good!

The even rows are the rows with the data for women.
These row values have a remainder of 0 when divided by 2.

```{r}
Women_BMI <- 
  tibble(country_split) %>% 
  dplyr::filter(row_number() %% 2 == 0)

head(Women_BMI)
```

Great! Now we have a list of the countries that can be used for both the male and female data. 

It's always a good idea to check that your data objects are the size you expect when wrangling. 
We can do so with the `dim()` function, which shows us the dimensions of data objects.

```{r}
dim(Women_BMI)
```
Great! There are 200 rows of character strings with the BMI data like we expected. 

We do however need to add `" Women"` back to this data.
To do this we will use the `glue()` function of the `glue` package. 

Here is a simple example of how this function glues two things together.

```{r}
where <-"the moon"

glue::glue("the cow jumped over {where}")

```

Now let's add  the word "Women" to each row of the `Women_BMI` data using `glue`.

First let's pull the `country_split` data out as a vector to make this simpler when we `glue`.
Then, let's use the `first()` function of the `dplyr` package to get a reminder of what just the first row of the `country_split` column of the `Women_BMI` data looks like.


```{r}
women_data <- Women_BMI %>%
  pull(country_split)

dplyr::first(women_data)
```

Now we will paste the word `"Women"` in the beginning of each value in the vector using the `glue()` function.

Note that there will be a space in between.
This is because the space is already in front of the first BMI value.

```{r}
Women <- glue::glue("Women{women_data}")
head(Women)
```

You could also do all of this in one step but its less obvious what is happening

Here we pull the `country_split` data out of the `Women_BMI` object and then paste `"Women"` in front of each string in one command
```{r}
# Women <- glue("Women{pull(Women_BMI,country_split)}")
```


Next, let's grab the male data:

```{r}
Men <- 
  tibble(rural_urban) %>% 
  dplyr::filter(row_number() %% 2 == 1) 
```
Remember our `rural_urban` object contains male data for the odd rows. 

```{r}
head(Men$rural_urban)
```

Again let's make sure we have the correct number of rows 
```{r}
dim(Men) 
```
How about our number of columns? 

If we try splitting our data by space again, will it have the expected number of columns? What about the rows that contain `na*` values?

Let's just take the `Men` data that contains `na*` values. 
This column is called `rural_urban`. 

```{r}
unknown <- 
  Men %>% 
  filter(str_detect(pattern ="na\\*", 
                    string = rural_urban)) 
```

Now we can try splitting by a space
```{r}
tibble::as_tibble(str_split(unknown$rural_urban, " ",
                            simplify = TRUE))

```

### Dealing with NA values

So close! Notice that the `"na*"` values have shifted the subsequent values within the columns because typically there is a space between the BMI and the credible intervals.
Here we can see this data in our original pdf:

```{r, echo = FALSE, out.width="100%"}
knitr::include_graphics(here::here("img","missing_pdf.png"))
```

We need to replace our `na*` values with something that includes a space so that when we separate our data by space we will have two values instead of one when we have an `na*`. 
Therefore, `na* na*` should work.

So, in order to use the functions within the `stringr` package, our `Men` data needs to be of `character` class, so let's check that now.

```{r}
class(Men$rural_urban)
```
The ` as.character()` function allows us to change factor data within  to the character class.  We could use this if we needed it.

We are also changing the structure so that the data is not within a column called `rural_urban`. 
```{r}
Men <- Men$rural_urban
```

```{r}
Men <- 
  stringr::str_replace_all(string = Men, 
                           pattern = "na\\*", 
                           replacement = "na\\* na\\*") 

# The \\ are necessary because the * is a special character 
# The * would typically indicate any possible value, 
# but here we actually want a "*" instead
# Thus the double backslash does that for us
# Here we are replacing all occurences of the na* values 
#(thus str_replace_all instead of str_replace) with na* na*. 
```

Let's check that it worked...
```{r}
Men[20:30]
```

Great!

Now for the Women data object
```{r}
class(Women)
# the female data is already character type
Women <- 
  stringr::str_replace_all(string = Women, 
                           pattern = "na\\*", 
                           replacement = "na\\* na\\*") 
Women[20:30]
```

Great, now we can split our data by spaces.

### Splitting the data

```{r}
Men <- tibble::as_tibble(str_split(Men, " ", 
                                   simplify = TRUE))
head(Men)
```
Note, here we need to use the column of `Men` that has the data

```{r}
Women <- as_tibble(str_split(Women, " ", 
                             simplify = TRUE))
head(Women)
Women[20:30,] 
```

We can see that our `na` values look correct. 

Looks good!
 
 
### Adding new header

We can see from our pdf and our object called `header` what the header was like in the original pdf document. We need to add column names to our data based on the original data.

Let' wait to add `Country` to the header.

```{r}
new_header <- c("Sex","National_BMI_1985", 
                "National_BMI_1985_CI", "Rural_BMI_1985", 
                "Rural_BMI_1985_CI", "Urban_BMI_1985",
                "Urban_BMI_1985_CI", "National_BMI_2017",
                "National_BMI_2017_CI","Rural_BMI_2017",
                "Rural_BMI_2017_CI", "Urban_BMI_2017", 
                "Urban_BMI_2017_CI")
```

Let's change the names of our columns of our tibbles to this new header for our `Men` and `Women` data

```{r}
names(Women) <- new_header
names(Men) <- new_header
```

Now we will add our country data to both our `Men` and `Women` tibbles using the `bind_cols()` function:

```{r}
Women <- dplyr::bind_cols(country, Women)
head(Women)
```
This will add the `country` as a new column to the `Women` data object on the left. 

Similarly, 
```{r}
Men <- dplyr::bind_cols(country, Men) 
```
This will add the `country` as a new column to the `Men` data object on the left. 

### Changing variable names

Here, we will change the variable name in the `country` dataset to `country` (currently it is called `country_split`). 
We will also introduce the concept of the assignment pipe. 
In this case our pipe operator looks like this `%<>%`. 
Using this additional pipe requires another package, called `magrittr`. 
The other simpler pipe options from this package are loaded with `tidyverse` (if you used `library(tidyverse)` which loads most tidyverse packages), but not this version. 

The `>` portion of the pipe still behaves like a normal pipe, while the `<` portion of the pipe makes an assignment to whatever the `<`is pointing to, just like when we use the typical assignment operator `<-`. 

```{r}
# library(magrittr) 
# We can't use the `%<>%` unless we load the magrittr package
# We have already done this but we include this for illustrative purposes.
# Here we will just use the traditional assignment strategy

Women <- dplyr::rename(Women, Country = country_split) 
```
Here, we have renamed the `country_split` variable to be called `Country`. 

Here, we reassign `Men` using the pipe strategy. 
```{r}
Men %<>% rename(Country = country_split) 
```
We have renamed the `country_split` variable to to be called `Country`. 
We have also reassigned `Men` to the same data object. which has the `Country` variable renamed. 

### Joining the data

Now, we can combine our `Men` and `Women` data using the `full_join()` function of `dplyr`. 
This will put all the male data first (`x`) and all the female data second (`y`).

```{r}
BMI <- dplyr::full_join(x = Men, y = Women)
```

Let's check the size of our BMI data... it should have 400 rows (obs).
```{r}
str(BMI)
```

### Sorting the data

Now, let's sort the data alphabetically by `Country` using the `arrange()` function from the `dplyr` package
```{r}
BMI <- dplyr::arrange(BMI, Country)
head(BMI)
```

Our data is looking great! 

Now, we might want to make sure that our observations for each variable look the way we want. 
In other words, if we want to make plots about National BMI in 1985 then we would need our values to be numeric. 
Looking at our BMI data using `str()`, we can see the type of data for each of our variables listed just after variable name and the `":"` colon. 

```{r, warning=FALSE, eval}
str(BMI)
```

Looks like none of our BMI data is actually `numeric` (or `num`), but of the class `character` (or `chr`). 
Let's change that now.

To start, we could change these values to be numeric with 6 lines of code like this:

```{r, eval = TRUE}
BMI$National_BMI_1985 %<>% as.numeric
BMI$Rural_BMI_1985 %<>% as.numeric
BMI$Urban_BMI_1985 %<>% as.numeric

BMI$National_BMI_2017 %<>% as.numeric
BMI$Rural_BMI_2017 %<>% as.numeric
BMI$Urban_BMI_2017 %<>% as.numeric
```

And if we did this we would see these variables show as `"num"` which stands for a numeric type of variable.
```{r}
str(BMI)
```

Or we can use a more automated way with the `map()` function of the `purrr` package:
```{r}
BMI_numeric <- 
  purrr::map((BMI %>% 
                select(-matches("CI|Sex|Country"))),
             as.numeric) %>% 
  as_tibble()
```

The `map()` function of the `purrr` package allows us to apply the `as.numeric()` function to all the selected columns of `BMI` dataset.
Above, we have selected those that don't contain `CI`, `Sex`, or `Country` in the column name. 
If you are familiar with `apply()`, this function is very similar.

```{r}
BMI_numeric %<>% 
  mutate(Country = pull(BMI, Country), 
         Sex = pull(BMI, Sex))
head(BMI_numeric)
```
In this case our numeric data is of a class `dbl`. 
This means double-precision floating point numbers (with decimals).
The alternative numeric option is the integer class. 

### Transforming to long format

It is generally useful to get the data in what is called <b>long</b> format for other analyses, and particularly for plotting. 

For a more detailed description about this, please see this [case study](https://opencasestudies.github.io/ocs-healthexpenditure/ocs-healthexpenditure.html).

Basically, the long format will have more rows and fewer columns.
Thus, we can combine or `"gather"` some columns together.
In our case, we can put all the different BMI data together in one long column.
We will create a new column that tells us what each row of the new BMI column represents.
In other words, it will tell us what the original column was.

To do this we will use the `gather()` function of the `tidyr` package:

```{r}
BMI_long <-
  tidyr::gather(data = BMI_numeric, 
                key = class_BMI, 
                value = BMI, 
                National_BMI_1985:Urban_BMI_2017, 
                factor_key = FALSE) 
```
The `data` value indicates what data you start with, the `key` indicates the new name of the column that will now contain information about the values of the columns we will put together. 
The `value` is the name of the column for the values for all the columns that are being put together. 
The `":"` can be used to identify the start and end of the column names that you are putting together. 
The `factor_key` argument determines if the newly created key column should be evaluated as a factor or not. 

```{r}
head(BMI_long)
```

It would be useful to parse the 1985 and the 2017 data. We can do so by separating the parts of the `class_BMI` column using the `separate()` function of the `tidyr` package. 
In our case we will replace the `class_BMI` column with two new columns.

```{r}
BMI_long %<>% 
  tidyr::separate(class_BMI, 
                  c("Region", NA, "Year")) 
```
Here, we separate the `class_BMI` column of the `BMI_long` tibble. 
This is based on the underscores and create two new columns. 
The first column will be called `Region`. 
It will contain the 1st part of the `class_BMI` data before the 1st underscore. 
The 2nd column will be called `Year`.
It will contain the 3rd part of the data based on underscores.
The middle part of the column will not be used to create any new columns. 

```{r}
head(BMI_long)
```

Let's see how the dimensions of the `BMI` data have changed in long format:
```{r}
dim(BMI_numeric)
dim(BMI_long)
```

#### {.question_block}
<u>Question opportunity:</u> Why exactly are there 2,400 rows now?

####

Great! our data is very usable now in this format!

```{r, echo = FALSE}
save(BMI_numeric, 
     BMI_long, 
     file = here("data", "Wrangled_data.rda"))

```

## Data Exploration

Now that our data is clean and in a format that we can work with, we can start to take a look at the data and how different groups might compare.

Going back to one of our original questions, 

> Is there a difference between rural and urban BMI estimates around the world?

We want to use a statistical test to identify if there are differences in the average BMI estimates between e.g. rural and urban groups. 
First, let's explore our data a bit. 

### General summary

```{r, echo = FALSE}
# Start here if not working through data import and wrangling
load(here("data", "Wrangled_data.rda"))
```

```{r}
summary(BMI_numeric)
```
We can see that the mean BMI estimates are larger in 2017.
However, is this a meaningful increase?
This is difficult to determine without a statistical test.

Lets look at the data stratified by `Sex`:

```{r}
BMI_numeric %>% 
  subset(Sex == "Women") %>% 
  summary()
```

```{r}
BMI_numeric %>% 
  subset(Sex == "Men") %>% 
  summary()
```

It looks like mean BMIs have increased in all regions for both men and women. It is unclear though if this change is statistically significant.

### Distributions

In order to apply a statistical test to compare the means, one of the first things to do is to explore the frequency of the different observed values. 
One way to summarize the frequency of different values is the <b>frequency distribution</b>, which can be shown in a table or a plot. 
See [here](http://onlinestatbook.com/2/introduction/distributions.html) for more information about distributions. 
To plot the frequency distribution of a dataset we can use the `hist()` function to create a histogram.

There are other types of distributions too, such as <b>probability distributions</b>, which describes the probability the occurrence of different possible outcomes. 
Probability distributions can be defined for discrete data (e.g. flipping a coin as heads or tails) and continuous data (e.g. BMI estimates). 
If we are talking about discrete data, the relative frequency of the different categories (heads or tails) can be used to define a <b>discrete probability distribution</b>. 
We can simply assign a probability to each category. 

<b>Continuous probability distributions</b> work in a similar way, except instead of assigning a probability to each category (e.g. heads or tails), we assign a probability to an interval of values (e.g. BMI between 22 and 26). 
This is because it's not useful to construct a distribution that assigns a probaiblity to each possible outcome (e.g. 22.11, 22.12, etc) with such high precision. In this case, we would have to assign a very small probability to every possible BMI. 

Now, the <b>normal (or Gaussian) distribution</b> is a type of <b>theortical continuous probability distribution</b> that is frequently used to approximate many naturally occurring distributions, including that of BMI. 
We say that a random quantity is normally distributed with average m and standard deviation s if its probability distribution is defined by:

**add equation here**

For example, if our data followed a <b>normal (or Gaussian) distribution</b> with mean of 24 and standard deviation of 1, then the probability of different outcomes should be equally centered around the mean and would look something like this:

```{r}
norm_BMI_ex_data <- 
  tibble(norm_data = rnorm(n = 200, mean = 24, sd = 1))
```

The `rnorm()` function allows us to make a vector of normally distributed data centered around the mean of 24. 

```{r}
head(norm_BMI_ex_data)
hist(norm_BMI_ex_data$norm_data)
```

Alternatively, we can plot this using the `geom_hist()` function of the `ggplot2` package. 
The [ggplot2](https://ggplot2.tidyverse.org/) package creates plots by using layers.
Notice in the following code how there is a plus sign between the `ggplot()` function and the `geom_histogram()` function. 
With `ggplot2` we select what data we would like to plot using the first function (`ggplot()`) and then we add on additional layers of complexity (these layers can even involve different data). 

```{r}
norm_BMI_ex_data %>% 
  ggplot2::ggplot(aes(x = norm_data)) + 
  ggplot2::geom_histogram() 
```

Here, we are using the column called `norm_data` within `norm_BMI_ex_data`.
As we noted, with `ggplot2` we must first specify the data to plot with the `ggplot()` function. 
Then, the `geom_*` function specifies what type of plot to create (e.g. `geom_histogram()` create a histogram). 

We will see later how we can add many layers to plots with `ggplot2`. For additional information on using `ggplot2`, see this [case study](https://opencasestudies.github.io/ocs-healthexpenditure/ocs-healthexpenditure.html).


Let's see how our data look:
```{r}
BMI_long %>% 
  ggplot(aes(x = BMI)) + 
  geom_histogram() 
```

OK, so the data looks like it is what we call <b>right-skewed</b> because the tail of the distribution is longer on the right side (in other words, it is a bit wider on that side), but it looks fairly centered. 
See [here](http://onlinestatbook.com/2/glossary/skew.html) for more information about skewed distributions.

If we split (or `facet`) our data by `Sex`, what do the frequency distributions look like? 
The easiest way to do this is to use some other functions of the `ggplot2` package, particularly the `facet_wrap()` function, which will allow us to look at differences in the distribution of the BMI estimates stratified by `Year`, `Sex`, and `Region`.
We can sequentially divide our plots by deeper levels using multiple variables and the `+` plus sign within `facet_wrap()`.

```{r, eval = TRUE}
BMI_long %>%
  ggplot(aes(x=BMI)) +
  geom_histogram() +
  facet_wrap(~ Sex)

BMI_long %>%
  ggplot(aes(x=BMI)) +
  geom_histogram() +
  facet_wrap(~ Sex + Year) 

BMI_long %>%
  ggplot(aes(x=BMI)) +
  geom_histogram() +
  facet_wrap(~ Sex + Year + Region) 

```

OK, so some of these plots look pretty similar to a normal distribution, like the `Women 2017 Urban` plot (although it is right-skewed).
However some of the other plots, like the `Men 2017 Urban` plot, look very different. 
These plots show what is called a [bimodal distribution](http://onlinestatbook.com/2/introduction/distributions.html), meaning that there appear to be two peaks (or modes), or in other words two different values (and values near each of these) that are the most frequent.

### Quantile-Quantile Plots

Let's use a plot called a "quantile-quantile" plot (or <b>Q-Q plots</b> for short) to determine if the data are indeed normally distributed. 
This plot allows us to compare two distributions: (1)  the fit of a known theoretical distribution (like the normal distribution) with our observed distribution. To do this we will plot the quantiles of our data on the y-axis and the quantiles of the theoretical normal distribution on the x-axis. If the quantiles line up then we can say that our data is fairly normal. What exactly is a <b>quantile</b>? This is a division of the data distribution into roughly equal portions. See [here](http://onlinestatbook.com/2/advanced_graphs/q-q_plots.html) for more information about Q-Q plots.

Here is an example of a Q-Q plot for the normally distributed data that we just created using the `stat_qq()` function of  the `ggplot2` package:

```{r}

norm_BMI_ex_data %>%
  ggplot(aes(sample =norm_data)) +
  stat_qq() + # to add the line we need to also include stat_qq_line()
  stat_qq_line()



```

Here we can see that the quantiles are fairly similar between the observed and theoretical data. We see that the points mostly fall on the line, however there are some points that are a bit further from the line as we get to the extreme quantiles. Notice that the sample quantiles (which will be fairly similar to our real BMI data quantiles) on the y-axis has the same range as the values that we created. So values that are bellow 22 for example are represented as the points bellow 22 on the y-axis. As expected we see that about half the points are bellow our mean of 24.

If we were to use different data that had a range of different values our y-axis would shift according to the range of values. For example the Orange data within the installation of R includes data about the circumference of orange trees in millimeters. Here we can see that the quantiles are quite different but reflect the range of orange tree circumferences.
```{r}
range(Orange$circumference)
Orange%>%
  ggplot(aes(sample =circumference)) +
  stat_qq() + # to add the line we need to also include stat_qq_line()
  stat_qq_line()
```


Let's take a look at our BMI data:

```{r}

BMI_long %>%
  ggplot(aes(sample =BMI)) +
  stat_qq() + # to add the line we need to also include stat_qq_line()
  stat_qq_line() +
  facet_wrap(~ Sex + Year + Region)
```
This is a little hard to see, some plots look pretty good, but while others seem to be skewed, lets take a closer look.

For the sake of simplicity, we are going to focus on the data from the women.  This is because women were identified in the paper to have larger increases in BMI. If we want to perform tests on these groups (women in each region and year) specifically, then we need to look at the distribution of each of these subsets.

```{r}

BMI_long %>%
  filter(Sex == "Women") %>%
  ggplot(aes(sample =BMI)) +
  stat_qq() + # to add the line we need to also include stat_qq_line()
  stat_qq_line() +
  facet_wrap(~ Year + Region)
```



We can see that at the extremes of our quantiles, for most of our data, our tails are not very similar to the theoretical distribution. We can can also determine that our data is right- or positive-skewed because the extreme values are above the line that indicates a perfect match between the theoretical and observed data.

 The rural data looks more normal than the urban data, but if we were to use statistical tests that rely on normality, this would require that both groups are normally distributed. 
 
Finally, some statisticians also use the [Shapiro-Wilk test for normality](http://www.statistics4u.info/fundstat_eng/ee_shapiro_wilk_test.html) when the Q-Q plot is a bit unclear. Many statisticians however would conclude from the Q-Q plots that normality appears to be violated in our data. 

### Shapiro-Wilk test

For illustrative purposes we will show how we can use the `dplyr` `summarize()` and `group_by()` functions to perform the Shapiro-Wilk test for all the subsets we are interested in.

```{r}

norm_BMI_ex_data$norm_data %>% 
  shapiro.test() 
# example of the test output for normal data - the p value is not < 0.05

BMI_long %>% 
  filter(Sex == "Women") %>% 
  group_by(Year, Region) %>% 
  summarize(shapiro_test = shapiro.test(BMI)$p.value)

```

We see that all the data does not appear to be normally distributed.

## Data Analysis

We can use statistical tests like the [student t-test](https://stattrek.com/statistics/dictionary.aspx?definition=two-sample%20t-test) to determine if [two means are significantly different](http://onlinestatbook.com/2/estimation/difference_means.html).

### Hypothesis testing
With the t-test, we perform what is called <b> two means hypothesis tests </b>.
We define what is called the <b> null hypothesis </b>, which is what we assume by default or the baseline, that there is no difference in the two means:

                                        Ho: μ1 = μ2
                                        
                               μ1 is the true mean of one group
                               μ2 is the true mean of the other group
                               
Thus like in law, we fail to reject the null this null hypothesis, unless we have enough evidence to suggest that we should reject it - similarly to the idea that we by default assume that individuals are not guilty until proven otherwise. If we reject the null hypothesis, we then accept an <b> alternative hypothesis </b>, this can vary for different statistical tests, but in the case of the t-test we evaluate if the two means are not equal:
                                        
                                        Ha: μ1 ≠ μ2
                                        
Remember that we can't know the true mean for our populations of interest, instead we approximate or estimate our means based on the sample that we have in our data.

When we use a statistical test to evaluate hypotheses like these, we use what we call the p-value. The p-value is a measure of the strength of the evidence for our null hypothesis. It is common practice to consider a p-value < 0.05 a strong enough evidence against the null hypothesis to reject it. Alternatively if the p-value > 0.5 then there is not enough evidence to reject the null hypothesis. We will use the p-values from our statistical tests to decide how to interpret the results of our tests.
    

We are interested in comparing the means of female rural and urban BMI measurements for both years. 

There are two possible classes of statistical tests that we could run to compare the means of these two groups:

1) Parametric
2) Nonparametric

Parametric tests are based on assumptions about the distribution of the data, while Nonparametric tests do not rely on this assumption. It is called "parametric" because aspects about the distribution of the data like the mean are called <b>parameters</b> when we describe a population. In parametric tests we estimate the parameters of the true population of interest using a sample of that population. These estimates are called <b>statistics</b>. See [here](https://www.mayo.edu/research/documents/parametric-and-nonparametric-demystifying-the-terms/doc-20408960) for more information about the difference between these two classes of tests.

### Parametric two sample mean tests

Often when comparing two groups we might perform a two sample t-test to determine if the means of each group is different. The two sample t-test however, relies on several <u><b>assumptions</b></u>:

1) The data for both groups is [normally distributed](http://onlinestatbook.com/2/introduction/distributions.html)
2) The [variance](https://stattrek.com/statistics/dictionary.aspx?definition=variance) of both groups is similar
3) The number of observations is similar for both groups - thus they are [balanced](https://www.statisticshowto.datasciencecentral.com/balanced-and-unbalanced-designs/)
4) The observations are [independent](https://www.stat.cmu.edu/~cshalizi/36-220/lecture-5.pdf) (meaning that observations do not influence each other)

If these assumptions are violated, this doesn't necessarily mean we can't perform a t-test. It just means we may need to consider the following options:

1) [Transformation](https://www.statisticshowto.datasciencecentral.com/transformation-statistics/) of the data to make it more normally distributed
2) [Welch's t-test](https://www.statisticshowto.datasciencecentral.com/welchs-test-for-unequal-variances/) also call the unequal variance t-test we may need to modify the way we perform the [t-test](https://stattrek.com/statistics/dictionary.aspx?definition=two-sample%20t-test) to account for the difference in the variance in the two groups
3) [Permutation/resampling methods](https://jhu-advdatasci.github.io/2019/lectures/21-resampling-techniques.html) to deal with violations of normality or imbalance.

Alternatively, we can use a nonparametric test which is model free and does not rely on assumptions about the parameters of the data. These tests are often a good option when multiple assumptions are violated are when sample sizes are small. We will explore these options.

Our data has a balance of observations for both groups - in fact they are equal, thus that assumption is not violated.
If it were violated, we would want to consider using permutation methods which are also a good option for violations of normality. To learn more about these methods see [here](https://jhu-advdatasci.github.io/2019/lectures/21-resampling-techniques.html).

If we needed to check if our samples were imbalanced, we could use the `count()` function of `dplyr`:

```{r}
dplyr::count(BMI_long, Sex, Year, Region)
```

We can see that the number of observations for each possible group of interest is the same.

The t-test is also fairly robust to non-normality if the data is relatively large, due to what is called the [central limit theorem](https://www.analyticsvidhya.com/blog/2019/05/statistics-101-introduction-central-limit-theorem/), which states that as samples get larger, they approximate a normal distribution.

We have an n of 200, which should be sufficient but let's investigate the nonparametric tests further. 


Often we would check if the variance of the rural and urban data is equal using the `var.test()` function. However this is an F test and assumes that the data is normally distributed. Instead we will use the `mood.test()` function which performs the [Mood's two-sample test for a difference in scale parameters](https://files.eric.ed.gov/fulltext/ED065559.pdf) and does not assume that the data is normally distributed. We will also introduce the `pull()` function of the `dplyr` package.

```{r}

#pull() will allow us to take the BMI column after we filter BMI_long

mood.test(dplyr::pull(filter(BMI_long, 
                             Sex == "Women", 
                             Year == "2017", 
                             Region == "Rural"), BMI), 
          dplyr::pull(filter(BMI_long,
                             Sex == "Women", 
                             Year == "2017", 
                             Region == "Urban"), BMI))
# p value <.05, conclude that variance is not equal
# reject the null: no difference in the spread of the distributions

mood.test(pull(filter(BMI_long, Sex == "Women", 
                      Year == "1985", 
                      Region == "Rural"), BMI), 
          pull(filter(BMI_long, 
                      Sex == "Women", 
                      Year == "1985", 
                      Region == "Urban"), BMI))
# p value <.05, conclude that variance is not equal
# reject the null: no difference in the variance of the distributions

mood.test(pull(filter(BMI_long, 
                      Sex == "Women", 
                      Year == "1985", 
                      Region == "Rural"), BMI), 
          pull(filter(BMI_long, Sex == "Women", 
                      Year == "2017", 
                      Region == "Rural"), BMI))
# p value >.05, conclude that variance is equal
# fail to reject the null the null: no difference in the variance of the distributions

mood.test(pull(filter(BMI_long, 
                      Sex == "Women", 
                      Year == "1985", 
                      Region == "Urban"), BMI), 
          pull(filter(BMI_long, Sex == "Women", 
                      Year == "2017", 
                      Region == "Urban"), BMI))
# p value >.05, conclude that variance is equal
# fail to reject the null the null: no difference in the variance of the distributions

```

Our p value is less than .05 for both tests, thus we reject our null hypothesis that there is no difference in the variance. Therefore, we conclude that the variance is not equal and that our data also violates this assumption.

We will perform a special t.test where we account for the fact that our variance is not equal. 

Another important consideration is that the data is what we call [paired](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5579465/), meaning that the measurements from the rural and urban areas are not independent. That is because we have a rural and urban measurement mean for nearly every country. Thus these values may be more similar to one another if they come from the same country. This is also true for the male and female measurements from the same country or the values in the same countries from 1985 and later in 2017. However, we are assuming that measurements between different countries are independent, thus this assumption is not violated, making it reasonable to perform the Welch's or paired t-test.

When we perform a paired t-test our hypothesis is slightly different from the typical student's t-test. In this case we are testing the differences among the pairs of observations and how close these differences are to zero. Our null hypothesis is that the mean of the differences is equal to zero:

                                        Ho: μd = 0
                                        
                               μd is the true mean differences 
                               between paired observations of the two groups

The alternative hypothesis is that the mean of the differences is not equal to zero

                                        Ha: μd ≠ 0
                                        
                               μd is the true mean differences 
                               between paired observations of the two groups

```{r}

t.test(pull(filter(BMI_long, Sex == "Women", 
                   Year == "2017", 
                   Region == "Rural"), BMI), 
       pull(filter(BMI_long, Sex == "Women", 
                   Year == "2017", 
                   Region == "Urban"), BMI), 
       var.equal = FALSE, paired = TRUE)
# means are different - p value <.05 reject the null: no difference in the means

t.test(pull(filter(BMI_long, Sex == "Women", 
                   Year == "1985",
                   Region == "Rural"), BMI), 
       pull(filter(BMI_long, Sex == "Women", 
                   Year == "1985", 
                   Region == "Urban"), BMI),
       var.equal = FALSE, paired = TRUE)
# means are different - p value <.05 reject the null: no difference in the means

t.test(pull(filter(BMI_long, Sex == "Women", 
                   Year == "1985", 
                   Region == "Rural"), BMI), 
       pull(filter(BMI_long, Sex == "Women", 
                   Year == "2017", 
                   Region == "Rural"), BMI),
       var.equal = TRUE, paired = TRUE)
# means are different - p value <.05 reject the null: no difference in the means

t.test(pull(filter(BMI_long, Sex == "Women", 
                   Year == "1985", 
                   Region == "Urban"), BMI), 
       pull(filter(BMI_long, Sex == "Women", 
                   Year == "2017", 
                   Region == "Urban"), BMI),
       var.equal = TRUE, paired = TRUE)
# means are different - p value <.05 reject the null: no difference in the means
# the t value shows us that the directionality of the difference
# the t value was negative thus BMI was lower in 1985 (1st group) 
# compared to 2017 (2nd group)


```

#### {.question_block}
<u>Question opportunity:</u> 
Looking at the t value, was global BMI lower in Rural or Urban areas in 1985?

####

Now we will try transform our data to make it more normally distributed. One way to do this is to take the logarithm of the data values. Then we will see how this influences the results. Again we will focus on the data for women.

```{r}

# create a new column with the log version of the BMI variable using mutate()

BMI_long_log <- mutate(BMI_long, log_BMI=log(pull(BMI_long,BMI)))

BMI_long_log %>%
  filter(Sex == "Women") %>%
  ggplot(aes(sample =log_BMI)) +
  stat_qq() + # to add the line we need to also include stat_qq_line()
  stat_qq_line() +
  facet_wrap(~ Year + Region)

BMI_long_log %>% 
  filter(Sex == "Women") %>% 
  group_by(Year, Region) %>% 
  summarize(shapiro_test = shapiro.test(log_BMI)$p.value)

# recall what it was before
BMI_long %>% 
  filter(Sex == "Women") %>% 
  group_by(Year, Region) %>% 
  summarize(shapiro_test = shapiro.test(BMI)$p.value)



```

The data appears to be more similar to the normal distribution, although not quite. Again, our sample size of 200 is quite large and the t-test is generally quite robust to violations of normality with large n, thus the modified t-test to account for unequal variance might be a good option using the log normalized data, as it is at least more normally distributed.

Let's see the results of the t-test with the transformed data:

```{r}

t.test(pull(filter(BMI_long_log, Sex == "Women", 
                   Year == "2017", 
                   Region == "Rural"), log_BMI), 
       pull(filter(BMI_long_log, Sex == "Women", 
                   Year == "2017", 
                   Region == "Urban"), log_BMI), 
       var.equal = FALSE, paired = TRUE)
# means are different - p value <.05 reject the null: no difference in the means

t.test(pull(filter(BMI_long_log, Sex == "Women", 
                   Year == "1985", 
                   Region == "Rural"), log_BMI), 
       pull(filter(BMI_long_log, Sex == "Women", 
                   Year == "1985", 
                   Region == "Urban"), log_BMI),
       var.equal = FALSE, paired = TRUE)
# means are different - p value <.05 reject the null: no difference in the means

t.test(pull(filter(BMI_long_log, Sex == "Women", 
                   Year == "1985", 
                   Region == "Rural"), log_BMI), 
       pull(filter(BMI_long_log, Sex == "Women", 
                   Year == "2017", 
                   Region == "Rural"), log_BMI),
       var.equal = TRUE, paired = TRUE)
# means are different - p value <.05 reject the null: no difference in the means

t.test(pull(filter(BMI_long_log, Sex == "Women", 
                   Year == "1985", 
                   Region == "Urban"), log_BMI), 
       pull(filter(BMI_long_log, Sex == "Women", 
                   Year == "2017", 
                   Region == "Urban"), log_BMI),
       var.equal = TRUE, paired = TRUE)
# means are different - p value <.05 reject the null: no difference in the means

```

We can see that our results are quite similar to that of the original data, however the t values are slightly smaller. In other cases we may see a much more dramatic influence of transforming our data.


Now, let's take a look at nonparametric tests, which are also a great option when the assumptions of the t-test are violated. 

### Nonparametric two sample tests

There are multiple nonparametric options to consider when the assumptions of the [t-test] are violated. The [Wilcoxon signed rank test](http://www.biostathandbook.com/wilcoxonsignedrank.html) (for paired data - the alternative is [Wilcoxon rank sum test](http://sphweb.bumc.bu.edu/otlt/mph-modules/bs/bs704_nonparametric/BS704_Nonparametric4.html) (also called the Mann-Whitney U test) for independent samples) and the [two-sample Kolmogorov-Smirnov test](https://www.itl.nist.gov/div898/software/dataplot/refman1/auxillar/ks2samp.htm) (KS) both do not assume normality (has both paired and unpaired methods). Thus these tests should be considered when the data of either groups does not appear to be normally distributed and particularly when the number of samples is low.

Importantly the KS test does not assume normality or equal variance, while the Wilcoxon signed rank test does assume equal variance. Here is how you would perform these tests. However in our case, because the variance is not equal between some of our groups of interest, the KS test would be more appropriate. Both the t-test and the KS test evaluate if the distributions of the two groups are identical, however the KS test does not particularly test any aspect of the distribution like the mean, therefore there are no confidence intervals in the output using this test. 

```{r}

ks.test(pull(filter(BMI_long, Sex == "Women", 
                    Year == "2017", 
                    Region == "Rural"), BMI), 
        pull(filter(BMI_long, Sex == "Women", 
                    Year == "2017", 
                    Region == "Urban"), BMI),
        paired = TRUE)

ks.test(pull(filter(BMI_long, Sex == "Women", 
                    Year == "1985", 
                    Region == "Rural"), BMI), 
        pull(filter(BMI_long, Sex == "Women", 
                    Year == "1985", 
                    Region == "Urban"), BMI),
        paired = TRUE)
```


What about the difference in female BMI from 1985 to 2017 for both regions?
Recall that the variance was equal for these comparisons.

```{r}

wilcox.test(pull(filter(BMI_long, Sex == "Women", 
                        Year == "1985", 
                        Region == "Rural"), BMI),
           pull(filter(BMI_long, Sex == "Women", 
                       Year == "2017", 
                       Region == "Rural"), BMI),
           paired = TRUE)

wilcox.test(pull(filter(BMI_long, Sex == "Women", 
                        Year == "1985", 
                        Region == "Urban"), BMI),
           pull(filter(BMI_long, Sex == "Women", 
                       Year == "2017", 
                       Region == "Urban"), BMI),
           paired = TRUE)

```
There is a significant difference across time for both regions, as we saw with the t-test. There is also a significant difference by region for each year. However, the p-values are a bit larger for the KS test results than we saw with the t-test.

### Multiple Testing Correction

Let's take a look back at the questions we wanted to answer:
1) Is there a difference between rural and urban BMI estimates around the world? In particular, how does this look for females?
2) How have BMI estimates changed from 1985 to 2017? Again, In particular, how does this look for females?
3) How do different countries compare for BMI estimates? In particular, how does the United States compare to the rest of the world?

It's important to note that ultimately we wanted to test 4 different tests to answer question 1 and 2:

1) Is there a difference in mean BMI for women between Rural and Urban areas in 1985?
2) Is there a difference in mean BMI for women between Rural and Urban areas in 2017?
3) Is there a difference in mean BMI for women between 1985 and 2017 in Rural areas?
4) Is there a difference in mean BMI for women between 1985 and 2017 in Urban areas?

When we test multiple questions like this, we need to correct for the multiple tests that we are performing.

The more we test data, the more likely we are to see a significant finding just by [random chance](https://www.stat.berkeley.edu/~mgoldman/Section0402.pdf) even when the finding is in fact not real. It all comes down to probability.

#### Probability

You may recall that probability is the chance that an event will happen and it ranges from 0 to 1. 
You can also think of it as a percentage of 0% (probability =0) to 100% (probability = 1) of chance that an event will happen.

![](https://www.mathsisfun.com/data/images/probability-line.svg)
https://www.mathsisfun.com/data/images/probability-line.svg

If we consider flipping a coin, the probability of an event happening (heads) is always equal to  1 ( or 100% chance) minus the probability of the event not happening (tails instead of heads) because the total probability can not exceed 100%.

        probability of 1 (100% chance) = the probabilty of heads (50% chance)
                                         + the probability of tails (50% chance)
                                       
                                     1 = P(heads) + P(tails)
            probability of one outcome = 1 - the probability of the other outcome
                              P(heads) = 1 - P(tails) 
                                        or
                              P(tails) = 1 - P(heads)

If we think about the probability of getting significant or nonsignificant results we can think of it like this:

                                     1 = P(signifiant results) 
                                          + P(no significant results)
                P(significant results) = 1 - P(no significant results)

### Alpha

Alpha is the amount of risk that you are willing to accept that a statistical test result is actually a false positive, in other words we reject the null hypothesis when the null hypothesis is true. This is also called [type 1 error](https://web.ma.utexas.edu/users/mks/statmistakes/errortypes.html). A significance threshold of .05 is customary and it means that we are accepting a 5% chance that our result is actually a false positive.

                                 alpha = probability of false significant results 
                                           or
                                     α = P(Making a type 1 error)
                                            
Remember that in our statistical tests if the p-value < alpha it is considered significant and if p-value > alpha it is considered not significant


But what really is the [p-value](https://towardsdatascience.com/p-values-explained-by-data-scientist-f40a746cfc8)?

### p-values

The p in p-value stands for <b>probability</b> - the probability that we would obtain the statistics (for example the t in our student t-tests based on the means of our groups of comparison) just by random chance alone. Therefore a p-value of 0.02 means that there is a 2 percent chance that out data may look the way it does just because of random chance and not because there is really a difference in the means of the groups of interest.

So then if alpha is the threshold for the p-value for obtaining false significant results then the probability of not making incorrect conclusions is 1 - alpha:

                 P(Not making an error) = 1 - α

```{r}
#P(Not making an error) = 1 - α
1-.05 

```
  
OK so if we use an alpha of .05 we are accepting that 95% of the time we will not make a Type 1 error and 5 % of the time we will just by random chance.              
Taking this one step further:

                     P(Making an error) = 1 - P(Not making an error)
                     P(Making an error) = 1 - (1 - α)
                                  
Here we can see that this checks out:                             
```{r}
#P(Making an error) = 1 - (1 - α)
1-(1-.05)
```

So what happens if we perform multiple tests?

The probability of not making a type 1 error would remain the same for each test. Therefore we need to multiply the probabilities together each time to determine the over all probability of making an error across multiple tests. See [here](http://mathforum.org/library/drmath/view/74065.html) about why we multiply probabilities together.

        P(Not making an error in m tests) = (1 - α)^m
    P(Making at least 1 error in m tests) = 1 - (1 - α)^m

Let's consider if we performed 10 different statistical tests and if we as usual considered the significance threshold alpha of .05:

So the probability of getting 1 significant result with 10 tests is:                                   
```{R}
#P(Making at least 1 error in 10 tests) = 1 - ((1 - α)(1 - α)(1 - α)(1 - α)(1 - α)(1 - α)(1 - α)(1 - α)(1 - α)(1 - α))
#this is the same as:
#P(Making at least 1 error in 10 tests) = 1 - (1 - α)^10
 1- (1-.05)^10
```

So there is a 40% chance that that there will be a significant finding simply due to random chance alone.

What about 100 tests?

```{R}

#P(Making at least 1 error in 100 tests) = 1 - (1 - α)^100
 1- (1-.05)^100
```
                        
Yikes!! That is almost a 100% chance that there will be a significant finding simply due to chance alone!

Much of this explanation is described in this [lecture](https://www.gs.washington.edu/academics/courses/akey/56008/lecture/lecture10.pdf)


One way to correct for this is multiple testing issue is using the [Bonferroni method](http://mathworld.wolfram.com/BonferroniCorrection.html). 

In this method we would divide our significance threshold (generally 0.05) by the number of tests.

```{r}
.05/4 # 4 tests
```

Our new significance threshold is now 0.0125. 
Thus our p-values should be less than this value for us to reject the null that there is no difference in means.
In all cases, our p-values were less than 0.0125.
So we see a significant difference in the means of the groups after multiple testing correction for our different tests.

Again, it would be reasonable to use the t-test because it is robust to deviations in normality when samples are relatively large. 
We can see that we obtained the same results regardless of the test that we used.
However, if sample sizes are small (generally speaking n<15 for each group), then these nonparametric options are useful to know.

The D values in the output of our KS tests, show the magnitude of distance in the difference between the distributions of the groups tested.
You may notice that the D value is larger for the tests of BMI across time rather then across region.
In these tests the p-value was also smaller.

## Data Visualization

Again we will utilize `ggplot2` to create plots to look at the directional and magnitude of the differences in BMI we are interested in. If you need additional information please see [here](https://opencasestudies.github.io/ocs-healthexpenditure/ocs-healthexpenditure.html). The top two lines of the code for the following plots, filter the data to only specific values of interest. Then we layer what is called a jitter on top of a box plot. A jitter is essentially a dot plot but with some variation on the location of the points so that they do not line up vertically which can make the individual points difficult to see.

Our first main question was: Is there a difference between rural and urban BMI estimates around the world?
Let's look at the national mean BMI estimates for each of the years:
```{r}
BMI_long %>% 
  filter(Sex == "Women", 
         Year == "1985", 
         Region != "National") %>%
  ggplot(aes(x = Region, y = BMI)) +
  geom_boxplot() +
  geom_jitter(width = .3) 
# the width determines how wide the points are plotted

BMI_long %>% 
  filter(Sex == "Women", 
         Year == "2017", 
         Region != "National") %>%
  ggplot(aes(x = Region, y = BMI)) +
  geom_boxplot() +
  geom_jitter(width = .3)

```

Our 2nd main question was: How have BMI estimates changed from 1985 to 2017?
Let's look at the change in rural and urban mean BMI estimates over time:
```{r}
#this time we will add titles for Rural and Urban using ggtitle()
BMI_long %>% 
  filter(Sex == "Women", 
         Year %in% c("1985", "2017"), 
         Region == "Rural") %>%
  # filtering this way allows us to keep data from both years
  ggplot(aes(x = Year, y = BMI)) +
  geom_boxplot() +
  geom_jitter(width = .3) +
  ggtitle("Rural")

BMI_long %>% 
  filter(Sex == "Women", 
         Year %in% c("1985", "2017"), 
         Region == "Urban") %>%
  ggplot(aes(x = Year, y = BMI)) +
  geom_boxplot() +
  geom_jitter(width = .3) +
  ggtitle("Urban")

```

Let's put the plots together to see how the change over the years differs between the regions.
We will use again use `facet_wrap()` to do this:

```{r}

BMI_long %>% 
  filter(Sex == "Women", 
         Year %in% c("1985", "2017"), 
         Region %in% c("Rural", "Urban")) %>%
  # filtering this way allows us to keep data from both years
  ggplot(aes(x = Year, y = BMI)) +
  geom_boxplot() +
  geom_jitter(width = .3) +
  facet_wrap(~ Region)

# we would also get national data without `Region %in% c("Rural", "Urban"))`

```
Indeed the change in BMI over time looks bigger in the rural areas!


One third main questions was: How do the different countries compare?  Or in other words what do the individual dots represent in our box plots?
We will take a look using `geom_label()`:

```{r}

BMI_long %>% 
  filter(Sex == "Women", 
         Year %in% c("1985", "2017"), 
         Region == "Rural") %>%
  ggplot(aes(x = Year, y = BMI)) +
  geom_boxplot() +
  geom_jitter(width = .3) +
  geom_label(aes(label = Country))

```

If we include all country names this is a bit too much... so perhaps we should focus on just the extreme BMI values using `filter()` function of the`dplyr` package. We will also use the `ggrepel` package to have our labels not overlap each other.

Part of the third question was: How does the United States of America compare? So let's label the data points for the United states.

```{r}

BMI_long %>% 
  filter(Sex == "Women", 
         Year %in% c("1985", "2017"), 
         Region == "Rural") %>%
  ggplot(aes(x = Year, y = BMI)) +
  geom_boxplot() +
  geom_jitter(data=BMI_long %>% 
              filter(Sex == "Women", 
                     Year %in% c("1985", "2017"), 
                     Region == "Rural") %>%
              filter(BMI>31 | BMI<19 | Country == "United States of America"), 
              aes(x =Year, y =BMI),
              width = .02) +
  ggrepel::geom_text_repel(data=BMI_long %>% 
              filter(Sex == "Women", 
                     Year %in% c("1985", "2017"), 
                     Region == "Rural") %>%
              filter(BMI>31 | BMI<19 | Country == "United States of America"), 
              aes(x =Year, y =BMI, label = Country))
              
              
```

And let's fill the box plots with color and outline in black:
```{r}

BMI_long %>% 
  filter(Sex == "Women", 
         Year %in% c("1985", "2017"), 
         Region == "Rural") %>%
  ggplot(aes(x = Year, y = BMI)) +
  geom_boxplot(outlier.shape = NA, color = "black", aes(fill = Year)) +
  geom_jitter(data=BMI_long %>% 
              filter(Sex == "Women", 
                     Year %in% c("1985", "2017"), 
                     Region == "Rural") %>%
              subset(BMI>31 | BMI<19 | Country == "United States of America"), 
              aes(x =Year, y =BMI),
              width = .02) +
  ggrepel::geom_text_repel(data=BMI_long %>% 
              filter(Sex == "Women", 
                     Year %in% c("1985", "2017"), 
                     Region == "Rural") %>%
              subset(BMI>31 | BMI<19 | Country == "United States of America"), 
              aes(x =Year, y =BMI, label = Country))

```


### Overall differences

Let's take a look at all the data together, including the data for men:
```{r}
ggplot(BMI_long, aes(x = Year, y = BMI, col = Region)) +
  geom_boxplot(outlier.shape = NA, color = "black" , aes(fill = Region)) + 
  facet_grid(~ Sex)   
```

That's useful, but let's look at the individual points and include our country labels, to do so lets change our United States of America label to USA:
```{r}
BMI_long$Country <-BMI_long$Country %>%
  str_replace( pattern = "United States of America", replacement = "USA")


ggplot(BMI_long, aes(x = Year, y = BMI, col = Year)) + 
  geom_boxplot(outlier.shape = NA, color = "black" , aes(fill = Year)) + 
  facet_grid(~ Sex + Region) + 
  geom_hline(yintercept=30, linetype="dashed", color = "red", size =1) + 
  # This will add a horizontal dashed line to indicate the obesity BMI threshold
  geom_jitter(data=subset(BMI_long), 
              aes(x =Year, y =BMI), 
              width = .2, size =2, shape =21, color = "black", fill = "gray") + 
  # This will add the individual country data points
  # The shape 21 allows for a different fill and outline color
  # The width determines how wide the jitter points are plotted
  geom_jitter(data=subset(BMI_long, Country == "USA"), 
              aes(x =Year, y =BMI), 
              width = .02, size =12, shape =21, color = "black", fill = "gray") + 
  # This will add points that are larger for the USA data
  geom_text(data=subset(BMI_long,  Country == "USA"), 
            aes(x =Year, y =BMI,label=Country), 
            color = "black") + 
  # This will add USA labels to the USA points
  theme(legend.position = "none", 
  # This is useful for removing the legend
        axis.text.x = element_text(size = 15,angle = 30), 
        # this changes the size and angle of the x axis point labels 
        axis.text.y = element_text(size = 20), 
        axis.title.y = element_text(size =15), 
        axis.title.x = element_text(size =15), 
        strip.text.x = element_text(size = 15)) +
  # This changes the size of x axis labels for the facet 
        ggtitle( "Differences in BMI Over Time and Across Region Type and Gender") 
  # Add a  plot title
   


```

Here we can see that overall BMI appears to be increasing globally over time. Additionally we can see that this is occurring not just in urban areas, but also in rural areas. The US is consistently above the median in all strata of the data. In general, the female data shows higher BMI values than the male data. The rural USA BMI appears to be higher than the urban BMI for both men and women. Many countries have average BMI estimates above the obesity threshold of 30. Thus it appears that education and outreach programs for weight management should focus on both rural and urban areas and both genders. Education and assistance for women may be especially helpful. 

### Differences in rate of change

How does the rate of change in BMI differ between groups? Which group might especially need attention?

First let's calculate the differences in BMI from 2017 and 1985 and add this to our BMI_long data object:
```{r}

BMI_numeric$Rural_difference <- 
     BMI_numeric$Rural_BMI_2017 - BMI_numeric$Rural_BMI_1985

BMI_numeric$Urban_difference <- 
     BMI_numeric$Urban_BMI_2017 - BMI_numeric$Urban_BMI_1985

BMI_numeric$National_difference <- 
     BMI_numeric$National_BMI_2017 - BMI_numeric$National_BMI_1985

BMI_diff_long <- BMI_numeric %>% 
  select(Country: National_difference) %>% 
gather(key = Type, value = Difference , Rural_difference:National_difference)

head(BMI_diff_long)
```

Let's replace "United states of America" with "USA" and make a plot with this data to compare the change in BMI:

```{r}

BMI_diff_long$Country <-BMI_diff_long$Country %>%
  str_replace( pattern = "United States of America", replacement = "USA")


ggplot(BMI_diff_long, aes(x = Type, y = Difference, col = Type)) + 
  geom_boxplot(outlier.shape = NA, color = "black" , aes(fill = Type)) + 
  facet_grid(~ Sex) +
  geom_jitter(data=subset(BMI_diff_long), 
              aes(x =Type, y =Difference), 
              width = .2, size =2, shape =21, color = "black", fill = "gray") + 
  # This will add the individual country data points
  # The shape 21 allows for a different fill and outline color
  # The width determines how wide the jitter points are plotted
  geom_jitter(data=subset(BMI_diff_long, Country == "USA"), 
              aes(x =Type, y =Difference), 
              width = .02, size =12, shape =21, color = "black", fill = "gray") + 
  # This will add points that are larger for the USA data
  geom_text(data=subset(BMI_diff_long,  Country == "USA"), 
            aes(x =Type, y =Difference,label=Country), color = "black") + 
  # This will add USA labels to the USA points
  theme(legend.position = "none", 
  # This is useful for removing the legend
        axis.text.x = element_text(size = 15,angle = 30), 
        # this changes the size and angle of the x axis point labels 
        axis.text.y = element_text(size = 20), 
        axis.title.y = element_text(size =15), 
        axis.title.x = element_text(size =15), 
        strip.text.x = element_text(size = 15)) +
        # this changes the size of x axis labels for the facet
        ggtitle( "Change in BMI Over Time and Across Region Type and Gender")
        # Add a plot title  



```

We can now see that the rate of change from 1985 to 2017 appears to be larger in the women compared to the men in all regions. The group with the largest increase in the USA is the women living in rural areas.

Let's check the difference with some statistical tests:

```{r}
#first lets look at the normality of the difference in BMI
BMI_diff_long %>%
  ggplot(aes(x=Difference)) +
  geom_histogram() +
  facet_wrap(~ Sex + Type)
# interesting...the spread of difference is much broader for women than for men 
# some women in some countries especially need help with obesity
# while other countries are actually showing a loss of BMI
# these countries are the points on our previous plot with a difference below 0

#Let's see who:
BMI_diff_long %>% 
  arrange(Difference)

# In general, people appear to have lost weight in Greece

# Let's look at Q-Q plots:
BMI_diff_long %>%
  ggplot(aes(sample =Difference)) +
  stat_qq() + # to add the line we need to also include stat_qq_line()
  stat_qq_line() +
  facet_wrap(~ Sex + Type)

# ok so some of the data for Men looks fairly normal but not as much for Women

# Now we will check the variance

mood.test(pull(filter(BMI_diff_long, 
                      Sex == "Women", 
                      Type == "Rural_difference"), Difference), 
          pull(filter(BMI_diff_long, 
                      Sex == "Women", 
                      Type == "Urban_difference"), Difference))
#reject the null - conclude that variance is not equal

mood.test(pull(filter(BMI_diff_long, 
                      Sex == "Men", 
                      Type == "Rural_difference"), Difference), 
          pull(filter(BMI_diff_long, 
                      Sex == "Men", 
                      Type == "Urban_difference"), Difference))
#fail to reject the null the null - conclude that variance is equal

mood.test(pull(filter(BMI_diff_long, 
                      Sex == "Women", 
                      Type == "Rural_difference"), Difference), 
          pull(filter(BMI_diff_long, 
                      Sex == "Men", 
                      Type == "Rural_difference"), Difference))
#reject the null - conclude that variance is not equal


# let's compare the groups
# this is again still paired data
# this is because each value of "Rural_difference" and "Urban_difference" correspond to the same country

ks.test(pull(filter(BMI_diff_long, 
                    Sex == "Women",  
                    Type == "Rural_difference"), Difference), 
        pull(filter(BMI_diff_long,
                    Sex == "Women",  
                    Type == "Urban_difference"), Difference),
        paired = TRUE) 

ks.test(pull(filter(BMI_diff_long,
                    Sex == "Men",
                    Type == "Rural_difference"), Difference), 
        pull(filter(BMI_diff_long,
                    Sex == "Men", 
                    Type == "Urban_difference"), Difference),
        paired = TRUE)

ks.test(pull(filter(BMI_diff_long,
                    Sex == "Men", 
                    Type == "Rural_difference"), Difference), 
        pull(filter(BMI_diff_long, 
                    Sex == "Women",  
                    Type == "Rural_difference"), Difference),
        paired = TRUE)

```
Now we have performed seven comparisons (4 earlier) so we should apply our multiple testing correction
```{r}
.05/7

```

We see that there is a significant difference in the change in BMI Rural communities between men and women. This change is larger for women.

Importantly- we noticed that it appears to be specific countries where BMI shows a particular increase especially for women. Which countries are those? how does that compare with the US? Clearly the US is among the countries with the highest differences. 

```{r}
BMI_diff_long %>% 
  filter(Country == "USA")

```

In the US, focus should be placed on <b> both urban and rural </b> women to improve this public health issue. 

Here we can see the countries have the largest differences in BMI from 1985-2017:

```{r}
BMI_diff_long %>%
  filter(Difference>4.9) %>%
  arrange(-Difference)
#here we sorted by the highest to lowest values of BMI Difference
```


However it is important to see what the mean BMI values are were for these countries in 2017. 
It could be that the average was underweight in 1985... let's take a look.

```{r}
BMI_long %>% 
  filter(Country %in% pull(filter(BMI_diff_long, Difference>4.9), Country), 
                    Sex %in% pull(filter(BMI_diff_long, Difference>4.9), Sex),
                    Year =="2017") %>%
  arrange(-BMI)

#Here is the data just for the changes in BMI in Egypt
filter(BMI_diff_long, Country == "Egypt") %>% 
  arrange(-Difference)

```
Thus rural women in Egypt had the greatest increase in BMI from 1985 to 2017 in this data (5.9) and the average BMI is now over the obesity threshold of 30.

The data suggests that rural women in Egypt and other countries may especially benefit from dietary resources and our interventions and programs to assist with weight management.

Now let's make a plot that summarizes our findings. To do this we will simplify the other plots we made and then combine them together.

```{r}

# simplified national means plot
Means_plot<-BMI_long %>% 
  filter(Sex %in% c("Men", "Women"), 
         Year %in% c("1985", "2017"), 
         Region == "National") %>%
ggplot(aes(x = Sex, y = BMI)) + 
  geom_boxplot(outlier.shape = NA, color = "black" , aes(fill = Sex)) + 
 scale_fill_manual(values=c("dodgerblue", "orchid2")) +
  facet_grid(~ Year) + 
  geom_hline(yintercept=30, linetype="dashed", color = "red", size =1) + 
  geom_jitter(data=BMI_long %>%
                filter(Sex %in% c("Men", "Women"), 
                       Year %in% c("1985", "2017"), 
                       Region == "National"),
                aes(x =Sex, y =BMI), 
                width = .2, size =2, shape =21, 
                color = "black", fill = "gray") +
  geom_jitter(data=subset(BMI_long %>%
                            filter(Sex %in% c("Men", "Women"), 
                                   Year %in% c("1985", "2017"), 
                                   Region == "National", 
                                   Country == "USA")), 
                             aes(x =Sex, y =BMI), 
                             width = .02, size =12, shape =21, 
                             color = "black", fill = "gray") + 
  # This will add points that are larger for the USA data
  geom_text(data=subset(BMI_long%>%
                          filter(Sex %in% c("Men", "Women"), 
                                 Year %in% c("1985", "2017"), 
                                 Region == "National", Country == "USA")),
                          aes(x =Sex, y =BMI,label=Country), 
                          color = "black") + 
  # This will add USA labels to the USA points
  theme_linedraw() +
  # This will make thebackground of the plot white and the facet labels black
  theme(legend.position = "none", 
  # This is useful for removing the legend
        axis.text.x = element_text(size = 15,angle = 30, vjust = 0.5), 
        # this changes the size and angle of the x axis point labels 
        axis.text.y = element_text(size = 20), 
        axis.title.y = element_text(size =15), 
        axis.title.x = element_text(size =15), 
        strip.text.x = element_text(size = 15))
   
#Simplified difference plot 
BMI_diff_long$Type <-BMI_diff_long$Type %>%
  str_replace( pattern = "_difference", replacement = "")

Diff_plot<-BMI_diff_long %>% 
  filter(Sex %in% c("Men", "Women"), 
         Type != "National") %>%
ggplot( aes(x = Type, y = Difference, col = Type)) + 
  geom_boxplot(outlier.shape = NA, color = "black" , aes(fill = Sex)) + 
  scale_fill_manual(values=c("dodgerblue", "orchid2")) +
  facet_grid(~ Sex) +
  geom_jitter(data=BMI_diff_long %>% 
    filter(Sex %in% c("Men", "Women"), 
           Type != "National"), 
    aes(x =Type, y =Difference), 
    width = .2, size =2, shape =21, 
    color = "black", fill = "gray") + 
  geom_jitter(data=subset(BMI_diff_long %>% 
    filter(Sex %in% c("Men", "Women"), 
           Type != "National", 
           Country == "USA")), 
    aes(x =Type, y =Difference), 
    width = .02, size =12, shape =21, 
    color = "black", fill = "gray") + 
  # This will add points that are larger for the USA data
  geom_text(data=subset(BMI_diff_long %>% 
    filter(Sex %in% c("Men", "Women"),
           Type != "National"), 
           Country == "USA"),
    aes(x =Type, y =Difference,label=Country), 
    color = "black") + 
  # This will add USA labels to the USA points
  theme_linedraw() +
    # This will make the background of the plot white and the facet labels black
  theme(legend.position = "none", 
  # This is useful for removing the legend
        axis.text.x = element_text(size = 15,angle = 30, vjust = 0.5), 
        # this changes the size and angle of the x axis point labels 
        axis.text.y = element_text(size = 20), 
        axis.title.y = element_text(size =15), 
        axis.title.x = element_text(size =15), 
        strip.text.x = element_text(size = 15))
  # this changes the size of x axis labels for the facet

#add labels
Diff_plot<-Diff_plot + 
        labs(title = "Change in BMI by region", 
             x = "", 
             y = "Change in BMI \n (1985 to 2017)") +
        theme(title = element_text (size = 12, face = "bold"))

Means_plot <-Means_plot + 
        labs(title = "Mean BMI over time", 
             x = "", 
             y = "Mean BMI") +
        theme(title = element_text (size = 12, face = "bold"))



obesity_text<-tibble(Year=c(1985),BMI=c(31),Sex=c("Men"),label=c("Obesity"))

Means_plot <-Means_plot + 
             geom_text(data = obesity_text,
                       label=pull(obesity_text,label), 
                       color = "red", 
                       aes( fontface ="bold.italic", size = 13))

cowplot::plot_grid(Means_plot, Diff_plot, labels = c("A", "B"))
```

We could make a similar plot with the `patchwork` package:
```{r}
Means_plot +labs(tag = 'A') + Diff_plot+ labs(tag = 'B') +
  plot_layout(guides = 'collect', ncol = 2)
```

Great, now we have put two plots together using the `plot_grid()` function of the `cowplot` package. This way we can clearly communicate two messages. The first being that BMI has increased over time globally and that many countries including the United States of America are approaching a mean BMI that is above the obesity threshold of 30. We can also see that women on average have larger BMI values than males, but that both genders show increased levels over time. In the second plot we can see that the increase in BMI is not just happening in  urban communities, but in both rural and urban communities and particularly in women. 

Our plot visually explores all of our main questions:
1) Is there a difference between rural and urban BMI estimates around the world? In particular, how does this look for females?
2) How have BMI estimates changed from 1985 to 2017? Again, In particular, how does this look for females?
3) How do different countries compare for BMI estimates? In particular, how does the United States compare to the rest of the world?


## Summary

We have evaluated BMI average estimates from 200 different countries around the world. To do so we imported data from a pdf using the `pdftools` package. We used `tidyverse` packages such as `dplyr`, `stringr`, and `tidy` to clean the data and get it in a workable format. Our statistical analysis focused on evaluating differences in BMI in females around the world across time and between rural and urban areas. We found a significant difference both between years and between the type of community among women globally using [t-tests](https://stattrek.com/statistics/dictionary.aspx?definition=two-sample%20t-test) and [nonparametric](https://www.mayo.edu/research/documents/parametric-and-nonparametric-demystifying-the-terms/doc-20408960) tests. Thus BMI has increased in women since 1985. Although BMI estimates are significantly higher in Urban areas compared to rural areas, BMI estimates have increased in both regions. We learned that there are assumptions and considerations to keep in mind when performing tests that compare two groups. 

<u> We learned that the t-test relies on:</u>

1) normality of the data for both groups (this is not as much of an issue if the number of observations is relatively large total n>30)
2) equal variance between the two groups (make sure you do the correct test if the data is not normal)
3) balanced sample sizes of the two groups
4) independent observations (or independent paired observations)

We learned that we can evaluate if our data is [normally distributed](https://www.physiology.org/doi/full/10.1152/advan.00064.2017) by plotting the distribution and by creating Q-Q plots.

<u>We learned that if our data is not normally distributed, we can consider these options:</u>

1) We can still perform a t-test if our n is large
2) We can transform the data before performing a t-test
3) We can use a nonparametric test (Wilcoxon signed rank test, the Wilcoxon rank sum test, and the Two-sample Kolmogorov-Smirnov (KS) test)
4) We can perform a t-test with resampling methods (which should be especially considered when the groups are imbalanced)

We learned that if our groups do not have equal variance, that we need to use a modified t-test to account for this or the correct nonparametric test that does not rely on this assumption (Two-sample Kolmogorov-Smirnov test).

We also learned that if our groups are not well balanced, that we can consider resampling methods.

Finally its also important to remember that when we have [paired data](https://www.statisticshowto.datasciencecentral.com/paired-data/) and intend to compare means, we should perform a paired test ([paired t-test](https://stattrek.com/hypothesis-test/paired-means.aspx#example1), Wilcoxon signed rank test, or paired KS test). Our data used in this case study was paired because observations were taken for the same countries for different categories of populations - thus, we wanted to compare these populations within the same country (male vs female, rural vs urban etc.). Other examples would be in a case-control study (if samples are matched for various demographics) or a study with repeated measures (for example, symptom measures from the same individual before and after a treatment). 


Using the `ggplot2` package we were able to visualize trends in the data. Importantly, the largest increase appears to be in the rural areas particularly for women. We were also able to identify how the US compared to other countries and which countries showed the largest increase in BMI over time. Analyses like this are important for defining which groups could benefit the most from interventions, education, and policy changes when attempting to mitigate public health challenges. You can see in the [article](https://www.nature.com/articles/s41586-019-1171-x.pdf) that this data came from that many additional considerations would be involved to perform such an analysis.


### Suggested Homework

Students can evaluate the change in BMI over time using the global data available for each year between 2015 and 2017. The data can be found [here](http://www.ncdrisc.org/downloads/bmi/NCD_RisC_Lancet_2017_BMI_age_standardised_world.csv).

### Helpful Links

[BMI](https://www.cdc.gov/healthyweight/assessing/bmi/adult_bmi/index.html)  
[Long and Wide Data Formats](https://opencasestudies.github.io/ocs-healthexpenditure/ocs-healthexpenditure.html)  
[Distributions](http://onlinestatbook.com/2/introduction/distributions.html)
[Normal Distribution](http://onlinestatbook.com/2/introduction/distributions.html)
[Skewed Distributions](http://onlinestatbook.com/2/glossary/skew.html)  
[Bimodal Distribution](http://onlinestatbook.com/2/introduction/distributions.html)  
[ggplot2](https://opencasestudies.github.io/ocs-healthexpenditure/ocs-healthexpenditure.html)    
[Q-Q Plots](http://onlinestatbook.com/2/advanced_graphs/q-q_plots.html)  
[Shapiro-Wilk Test](http://www.statistics4u.info/fundstat_eng/ee_shapiro_wilk_test.html)  
[Student t-test](https://stattrek.com/statistics/dictionary.aspx?definition=two-sample%20t-test)   
[Paired Data](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5579465/)  
[Welch's t-test](https://www.statisticshowto.datasciencecentral.com/welchs-test-for-unequal-variances/)    
[Parametric and Nonparametric Methods](https://www.mayo.edu/research/documents/parametric-and-nonparametric-demystifying-the-terms/doc-20408960)  
[Variance](https://stattrek.com/statistics/dictionary.aspx?definition=variance)  
[Balanced Study Design](https://www.statisticshowto.datasciencecentral.com/balanced-and-unbalanced-designs/)  
[Independent Observations](https://www.stat.cmu.edu/~cshalizi/36-220/lecture-5.pdf)  
[Transformation](https://www.statisticshowto.datasciencecentral.com/transformation-statistics/)  
[Permutation/Resampling Methods](https://jhu-advdatasci.github.io/2019/lectures/21-resampling-techniques.html)   
[Central Limit Theorem](https://www.analyticsvidhya.com/blog/2019/05/statistics-101-introduction-central-limit-theorem/)  
[Mood's Two-Sample Scale Test](https://files.eric.ed.gov/fulltext/ED065559.pdf)  
[Wilcoxon Signed Rank Test](http://www.biostathandbook.com/wilcoxonsignedrank.html)   
[Wilcoxon Rank Sum Test](http://sphweb.bumc.bu.edu/otlt/mph-modules/bs/bs704_nonparametric/BS704_Nonparametric4.html)  
[Two-sample Kolmogorov-Smirnov Test](https://www.itl.nist.gov/div898/software/dataplot/refman1/auxillar/ks2samp.htm)  
[Type 1 Error](https://web.ma.utexas.edu/users/mks/statmistakes/errortypes.html)  
[p-value](https://towardsdatascience.com/p-values-explained-by-data-scientist-f40a746cfc8)  
[Multiple Testing](https://www.gs.washington.edu/academics/courses/akey/56008/lecture/lecture10.pdf)    
[Bonferroni Method of Multiple Testing Correction](http://mathworld.wolfram.com/BonferroniCorrection.html)



